{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit learn에서 제공하는 data set을 사용하기 위한 모듈\n",
    "from sklearn import datasets\n",
    "# dicision tree 기계학습 모델을 사용하기 위한 모듈\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# data set을 train set과 test set으로 분리할 수 있는 클래스를 제공하는 모듈\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Stratiffied k fold cross validation을 사용하기 위한 모듈 Kfold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# cross validation 결과의 정확도를 측정하기 위한 모듈\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분석결과의 confusion matrix를 추출하기 위한 모듈\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# 분석결과의 accuracy를 측정하기 위한 모듈\n",
    "from sklearn.metrics import accuracy_score\n",
    "# 분석결과의 recall, precision, f-measure를 측정하기 위한 모듈\n",
    "from sklearn.metrics import classification_report\n",
    "# roc 곡선 아래 넓이를 구하기 위한 모듈\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# 분석결과의 MSE를 구하기 위한 모듈\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets에 있는 load_breast_cancer 함수를 사용해 유방암 데이터를 가져와 변수 data에 저장\n",
    "data = datasets.load_breast_cancer()\n",
    "# 유방암 데이터 중 속성 데이터를 변수 X에 저장\n",
    "x = data.data\n",
    "# 유방암 데이터 중 클래스 데이터를 변수 y에 저장\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_test_split 함수를 사용해 train set과 test set을 분리하고, test_size를 전체 데이터의 20%로 설정\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DecisionTreeClassifier 모델을 변수 clf에 할당\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train data(x_train, y_train)와 fit 함수를 사용해 모델 훈련\n",
    "clf.fit(x_train, y_train)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict 함수를 사용해 test data()에 대한 예측값을 구하고 변수 y_pred에 저장\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[57  8]\n",
      " [ 2 47]]\n"
     ]
    }
   ],
   "source": [
    "# test data 실제값인 y_test와 예측 결과값 y_pred를 confusion_matrix 함수에 입력해 matrix 출력\n",
    "print('Confusion Matrix')\n",
    "# labels는 진짜, 가짜를 순서 바꾸어주기\n",
    "print(confusion_matrix(y_test, y_pred, labels = [1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "0.9122807017543859\n"
     ]
    }
   ],
   "source": [
    "# y_test와 y_pred 값을 비교해 정확도(accuracy)를 출력\n",
    "print('Accuracy')\n",
    "# normalize=False: 올바르게 분류된 데이터 건수 출력\n",
    "# normalize=True: 올바르게 분류된 데이터의 비율 출력\n",
    "print(accuracy_score(y_test, y_pred, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90        49\n",
      "           1       0.97      0.88      0.92        65\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       114\n",
      "   macro avg       0.91      0.92      0.91       114\n",
      "weighted avg       0.92      0.91      0.91       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 강의자료 p.33\n",
    "# Classification_report 함수를 사용해 각 클래스에 대한 precision, recall 출력\n",
    "print('Classification Report')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC\n",
      "0.9180533751962323\n"
     ]
    }
   ],
   "source": [
    "# Roc_auc_score 함수를 사용해 roc 곡선 아래 면적 출력\n",
    "print('AUC')\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_Squared Error\n",
      "0.08771929824561403\n"
     ]
    }
   ],
   "source": [
    "# Mean_squared_error 함수를 사용해 MSE 출력\n",
    "print('Mean_Squared Error')\n",
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K fold cross validation\n",
    " 데이터 셋 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits = 10)\n",
    "skf.get_n_splits(x, y)\n",
    "print(skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-60-2c65345d29ad>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-60-2c65345d29ad>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    for tarin_index, test_index in skf.split(x, y)\u001b[0m\n\u001b[1;37m                                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 10번의 실험을 위한 데이터 셋 구성을 살펴봄\n",
    "for tarin_index, test_index in skf.split(x, y)\n",
    "    print('Train set: ', train_index)\n",
    "    print('Test set: ', test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Fold Cross Validation Score\n",
      "[0.9137931  0.87931034 0.9122807  0.84210526 0.96491228 0.87719298\n",
      " 0.85964912 0.94642857 0.91071429 0.92857143]\n",
      "Average Accuracy\n",
      "0.9034958084867342\n"
     ]
    }
   ],
   "source": [
    "# 기계학습 모델 성능평가\n",
    "# DecisionTreeClassifier 모델을 clf에 생성\n",
    "clf = DecisionTreeClassifier()\n",
    "# Cross_val_score 함수를 사용해 x, y 데이터에 대해 10 fold cross validation 진행한 accuracy 출력\n",
    "scores = cross_val_score(clf, x, y, cv = skf)\n",
    "# 10개 accuracy의 평균 출력\n",
    "print('K Fold Cross Validation Score')\n",
    "print(scores)\n",
    "print('Average Accuracy')\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K fold cross validation - shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=None, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "# StratifiedKFold 모듈을 사용해 10 cross validation 모듈을 skf로 생성\n",
    "# Shuffle을 True로 설정하여 데이터를 섞은 후 데이터 셋 구성하도록 함\n",
    "skf_sh = StratifiedKFold(n_splits = 10, shuffle = True)\n",
    "skf_sh.get_n_splits(x, y)\n",
    "print(skf_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  [  0   1   3   4   5   6   8   9  10  11  13  14  15  16  17  18  19  20\n",
      "  21  22  23  24  25  26  27  30  31  32  33  34  35  36  37  38  40  41\n",
      "  42  43  44  45  46  47  48  49  50  51  52  54  55  56  57  58  59  60\n",
      "  61  62  63  65  67  68  69  70  71  72  73  74  75  77  78  79  80  81\n",
      "  82  83  84  86  87  88  89  91  92  93  94  96  97  99 100 101 102 103\n",
      " 104 105 106 107 108 111 112 113 114 115 116 118 119 120 121 122 123 125\n",
      " 126 127 129 130 131 132 133 134 135 136 137 138 139 141 142 143 144 145\n",
      " 146 147 148 149 150 151 152 153 154 155 156 157 159 160 161 162 163 164\n",
      " 166 167 168 169 171 172 173 174 175 176 178 179 180 181 182 183 184 185\n",
      " 186 187 188 190 191 192 193 194 195 196 198 199 200 201 202 203 204 207\n",
      " 208 209 210 211 212 213 214 216 217 218 219 220 221 222 223 224 225 226\n",
      " 227 229 230 231 232 233 235 236 237 238 239 240 241 242 243 244 245 247\n",
      " 248 249 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266\n",
      " 268 269 270 271 272 273 275 276 277 278 279 280 281 282 283 284 285 286\n",
      " 287 288 290 291 292 293 294 295 296 297 298 300 301 302 303 304 305 306\n",
      " 307 308 309 310 312 313 314 315 316 317 318 319 320 322 323 324 325 326\n",
      " 327 328 329 330 331 332 333 334 335 338 339 340 341 342 343 344 345 346\n",
      " 347 348 349 350 351 353 354 356 357 358 359 360 361 362 363 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385\n",
      " 386 388 389 390 391 392 393 394 395 396 397 398 399 400 402 403 404 405\n",
      " 406 407 409 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425\n",
      " 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 444\n",
      " 445 446 447 448 449 450 451 452 453 454 455 456 457 458 460 461 462 463\n",
      " 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481\n",
      " 482 483 485 487 488 489 490 492 493 494 495 496 497 499 500 501 502 503\n",
      " 505 506 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523\n",
      " 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543\n",
      " 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561\n",
      " 562 563 564 565 566 567 568]\n",
      "Test set:  [  2   7  12  28  29  39  53  64  66  76  85  90  95  98 109 110 117 124\n",
      " 128 140 158 165 170 177 189 197 205 206 215 228 234 246 250 267 274 289\n",
      " 299 311 321 336 337 352 355 364 387 401 408 410 443 459 484 486 491 498\n",
      " 504 507 524 525]\n",
      "Train set:  [  0   2   3   5   6   7   8  10  11  12  13  14  15  16  17  18  19  20\n",
      "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  37  38  39\n",
      "  40  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  59  60\n",
      "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "  79  80  81  82  83  84  85  86  87  88  89  90  91  92  94  95  96  97\n",
      "  98  99 100 101 102 103 104 105 106 107 109 110 111 112 113 115 116 117\n",
      " 118 119 120 121 122 124 125 127 128 129 131 132 133 134 135 137 139 140\n",
      " 141 142 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159\n",
      " 160 161 162 164 165 166 167 168 169 170 171 172 173 174 175 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 197 198\n",
      " 199 200 201 202 203 204 205 206 207 208 210 212 213 214 215 216 217 218\n",
      " 220 221 222 223 224 225 226 227 228 229 230 231 232 234 235 236 237 238\n",
      " 239 240 241 242 243 244 245 246 247 248 250 251 252 253 254 255 256 257\n",
      " 258 259 260 261 262 265 266 267 268 269 270 271 272 273 274 275 276 277\n",
      " 278 279 280 281 282 283 284 285 286 287 288 289 290 291 293 294 295 296\n",
      " 298 299 300 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322 323 324 325 326 327 328 329 331 332 333 334 336\n",
      " 337 338 339 340 341 342 344 345 346 347 348 349 350 351 352 353 354 355\n",
      " 356 357 358 360 361 362 363 364 366 367 368 369 371 372 373 375 376 377\n",
      " 378 379 380 381 382 383 385 387 388 390 391 392 393 394 395 396 397 398\n",
      " 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416\n",
      " 417 418 419 420 421 423 425 426 427 428 429 430 431 432 434 435 436 437\n",
      " 439 441 442 443 444 445 446 447 448 449 450 451 452 454 456 457 458 459\n",
      " 460 461 462 463 464 465 466 467 468 469 470 472 473 474 475 476 477 478\n",
      " 479 480 482 483 484 485 486 487 489 490 491 492 493 494 496 497 498 499\n",
      " 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517\n",
      " 518 519 521 522 523 524 525 526 527 528 529 530 531 533 534 535 536 537\n",
      " 538 539 540 541 542 544 545 546 548 550 552 554 555 556 557 558 559 560\n",
      " 561 563 564 565 566 567 568]\n",
      "Test set:  [  1   4   9  36  41  50  58  93 108 114 123 126 130 136 138 143 163 176\n",
      " 196 209 211 219 233 249 263 264 292 297 301 330 335 343 359 365 370 374\n",
      " 384 386 389 422 424 433 438 440 453 455 471 481 488 495 520 532 543 547\n",
      " 549 551 553 562]\n",
      "Train set:  [  0   1   2   3   4   5   7   9  10  11  12  13  14  15  16  17  18  19\n",
      "  20  21  22  23  24  26  27  28  29  30  31  32  33  34  35  36  37  39\n",
      "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
      "  60  61  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "  79  80  82  83  84  85  86  87  89  90  91  92  93  94  95  96  97  98\n",
      "  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 117\n",
      " 118 119 120 121 122 123 124 125 126 127 128 129 130 132 133 135 136 137\n",
      " 138 140 141 143 144 146 147 148 149 150 151 152 153 155 156 157 158 159\n",
      " 160 161 162 163 164 165 166 167 168 169 170 172 173 174 176 177 178 179\n",
      " 180 182 183 184 185 186 187 188 189 190 192 193 194 195 196 197 198 199\n",
      " 200 201 202 203 204 205 206 207 208 209 210 211 212 214 215 216 217 218\n",
      " 219 220 221 222 224 226 227 228 230 231 232 233 234 235 236 238 239 240\n",
      " 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 257 258 259\n",
      " 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277\n",
      " 280 282 283 284 285 287 288 289 290 291 292 293 294 295 296 297 298 299\n",
      " 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318\n",
      " 319 320 321 322 323 325 326 327 328 329 330 331 333 334 335 336 337 338\n",
      " 339 340 341 342 343 345 346 347 348 349 350 351 352 353 354 355 356 357\n",
      " 359 360 362 363 364 365 366 367 368 369 370 371 372 373 374 376 377 378\n",
      " 380 381 383 384 385 386 387 388 389 392 393 394 395 396 397 398 400 401\n",
      " 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 419 420\n",
      " 421 422 423 424 425 427 428 429 430 432 433 434 435 436 437 438 439 440\n",
      " 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458\n",
      " 459 460 461 462 463 464 465 466 467 468 470 471 472 475 476 477 478 479\n",
      " 480 481 482 483 484 485 486 488 489 490 491 492 493 494 495 496 497 498\n",
      " 499 500 501 502 503 504 507 508 509 510 511 512 513 514 515 516 517 518\n",
      " 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 537\n",
      " 538 539 540 541 542 543 544 545 546 547 549 550 551 553 554 555 556 558\n",
      " 559 560 561 562 564 566 567 568]\n",
      "Test set:  [  6   8  25  38  40  59  62  81  88 116 131 134 139 142 145 154 171 175\n",
      " 181 191 213 223 225 229 237 256 278 279 281 286 300 324 332 344 358 361\n",
      " 375 379 382 390 391 399 418 426 431 469 473 474 487 505 506 536 548 552\n",
      " 557 563 565]\n",
      "Train set:  [  0   1   2   4   5   6   7   8   9  10  11  12  13  14  15  17  18  19\n",
      "  20  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37  38\n",
      "  39  40  41  42  45  46  47  48  49  50  52  53  54  55  56  57  58  59\n",
      "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  76  77  79  80\n",
      "  81  83  85  86  87  88  89  90  91  92  93  94  95  97  98  99 100 101\n",
      " 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
      " 120 121 123 124 125 126 127 128 129 130 131 132 133 134 136 137 138 139\n",
      " 140 141 142 143 145 147 148 149 150 152 153 154 155 156 157 158 159 160\n",
      " 161 162 163 165 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 199 200 201 202 204 205 206 207 208 209 211 212 213 214 215 216 217 218\n",
      " 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236\n",
      " 237 238 239 240 241 242 243 244 245 246 247 249 250 251 253 254 255 256\n",
      " 257 258 259 260 261 262 263 264 265 267 268 269 270 271 272 273 274 275\n",
      " 276 277 278 279 280 281 282 283 284 285 286 287 289 290 291 292 293 294\n",
      " 295 296 297 298 299 300 301 302 303 305 307 308 310 311 312 313 315 317\n",
      " 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336\n",
      " 337 338 339 340 342 343 344 346 348 349 350 351 352 353 354 355 356 357\n",
      " 358 359 360 361 362 363 364 365 367 368 369 370 371 372 373 374 375 376\n",
      " 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 395\n",
      " 397 398 399 400 401 402 404 405 407 408 409 410 411 413 415 417 418 419\n",
      " 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438\n",
      " 440 441 442 443 444 445 446 447 448 450 451 452 453 454 455 456 457 458\n",
      " 459 460 461 462 464 465 467 468 469 470 471 472 473 474 475 477 478 479\n",
      " 481 482 483 484 485 486 487 488 489 490 491 493 494 495 496 497 498 499\n",
      " 500 501 502 503 504 505 506 507 508 510 511 512 513 514 515 516 517 518\n",
      " 520 521 522 523 524 525 526 527 529 530 531 532 533 534 535 536 537 539\n",
      " 540 542 543 544 545 546 547 548 549 551 552 553 554 555 556 557 558 559\n",
      " 560 561 562 563 564 565 567 568]\n",
      "Test set:  [  3  16  26  43  44  51  60  75  78  82  84  96 122 135 144 146 151 164\n",
      " 166 203 210 248 252 266 288 304 306 309 314 316 318 341 345 347 366 394\n",
      " 396 403 406 412 414 416 420 439 449 463 466 476 480 492 509 519 528 538\n",
      " 541 550 566]\n",
      "Train set:  [  0   1   2   3   4   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  24  25  26  27  28  29  30  31  33  34  35  36  37  38  39\n",
      "  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
      "  58  59  60  61  62  63  64  65  66  68  69  70  71  73  74  75  76  77\n",
      "  78  79  81  82  83  84  85  86  87  88  90  91  93  94  95  96  98  99\n",
      " 100 101 102 103 104 107 108 109 110 111 112 114 115 116 117 118 119 120\n",
      " 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138\n",
      " 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156\n",
      " 157 158 159 161 162 163 164 165 166 167 168 170 171 172 174 175 176 177\n",
      " 178 179 180 181 182 183 184 185 186 188 189 191 192 194 195 196 197 198\n",
      " 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n",
      " 217 219 220 221 222 223 224 225 227 228 229 231 233 234 235 237 238 239\n",
      " 240 244 245 246 248 249 250 252 253 254 255 256 257 258 259 261 262 263\n",
      " 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 281 282\n",
      " 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300\n",
      " 301 302 303 304 305 306 307 309 310 311 312 313 314 315 316 317 318 319\n",
      " 320 321 322 324 325 326 327 328 330 331 332 333 334 335 336 337 338 339\n",
      " 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357\n",
      " 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 374 375 376\n",
      " 377 378 379 380 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 398 399 400 401 402 403 405 406 407 408 410 411 412 414 415 416 417\n",
      " 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435\n",
      " 437 438 439 440 442 443 444 445 447 448 449 450 452 453 454 455 457 458\n",
      " 459 460 461 463 464 466 469 470 471 472 473 474 475 476 477 478 479 480\n",
      " 481 482 483 484 486 487 488 489 490 491 492 493 494 495 496 497 498 499\n",
      " 500 501 503 504 505 506 507 508 509 510 511 512 515 516 517 518 519 520\n",
      " 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 538 539\n",
      " 540 541 543 544 545 546 547 548 549 550 551 552 553 554 555 557 558 560\n",
      " 561 562 563 564 565 566 567 568]\n",
      "Test set:  [  5  22  23  32  67  72  80  89  92  97 105 106 113 160 169 173 187 190\n",
      " 193 218 226 230 232 236 241 242 243 247 251 260 280 308 323 329 373 381\n",
      " 397 404 409 413 436 441 446 451 456 462 465 467 468 485 502 513 514 537\n",
      " 542 556 559]\n",
      "Train set:  [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  16  19  20  21\n",
      "  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
      "  40  41  42  43  44  45  46  47  48  49  50  51  52  53  55  56  58  59\n",
      "  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77\n",
      "  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95\n",
      "  96  97  98  99 101 102 103 105 106 107 108 109 110 112 113 114 115 116\n",
      " 117 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135\n",
      " 136 137 138 139 140 141 142 143 144 145 146 147 148 149 151 153 154 155\n",
      " 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173\n",
      " 174 175 176 177 178 180 181 182 183 184 185 186 187 189 190 191 192 193\n",
      " 194 195 196 197 198 200 201 202 203 204 205 206 208 209 210 211 212 213\n",
      " 214 215 216 218 219 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 241 242 243 244 245 246 247 248 249 250 251 252 253 254\n",
      " 255 256 257 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273\n",
      " 274 276 277 278 279 280 281 283 285 286 287 288 289 291 292 293 294 295\n",
      " 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 314\n",
      " 315 316 318 320 321 322 323 324 325 326 327 328 329 330 332 333 335 336\n",
      " 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 355\n",
      " 357 358 359 360 361 363 364 365 366 367 369 370 371 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 408 409 410 411 412 413 414\n",
      " 415 416 417 418 420 421 422 424 425 426 427 429 431 432 433 434 435 436\n",
      " 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454\n",
      " 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 471 472 473\n",
      " 474 475 476 477 478 479 480 481 482 484 485 486 487 488 489 490 491 492\n",
      " 494 495 497 498 501 502 503 504 505 506 507 509 511 512 513 514 515 517\n",
      " 519 520 521 522 523 524 525 526 527 528 530 531 532 534 536 537 538 541\n",
      " 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559\n",
      " 560 562 563 564 565 566 567 568]\n",
      "Test set:  [  0  15  17  18  54  57 100 104 111 118 150 152 179 188 199 207 217 220\n",
      " 238 239 240 258 275 282 284 290 313 317 319 331 334 354 356 362 368 372\n",
      " 407 419 423 428 430 470 483 493 496 499 500 508 510 516 518 529 533 535\n",
      " 539 540 561]\n",
      "Train set:  [  0   1   2   3   4   5   6   7   8   9  11  12  15  16  17  18  19  20\n",
      "  21  22  23  25  26  27  28  29  30  31  32  33  35  36  37  38  39  40\n",
      "  41  42  43  44  45  46  49  50  51  52  53  54  56  57  58  59  60  61\n",
      "  62  63  64  65  66  67  70  72  74  75  76  77  78  79  80  81  82  83\n",
      "  84  85  86  87  88  89  90  91  92  93  95  96  97  98  99 100 101 104\n",
      " 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 121 122 123\n",
      " 124 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 142 143\n",
      " 144 145 146 148 149 150 151 152 153 154 155 156 157 158 160 161 163 164\n",
      " 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 184\n",
      " 185 187 188 189 190 191 193 194 195 196 197 198 199 200 201 202 203 204\n",
      " 205 206 207 209 210 211 213 214 215 217 218 219 220 221 222 223 224 225\n",
      " 226 228 229 230 232 233 234 235 236 237 238 239 240 241 242 243 244 245\n",
      " 246 247 248 249 250 251 252 253 254 255 256 257 258 260 261 262 263 264\n",
      " 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282\n",
      " 283 284 286 287 288 289 290 291 292 293 294 295 297 298 299 300 301 302\n",
      " 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320\n",
      " 321 323 324 325 326 327 329 330 331 332 333 334 335 336 337 338 340 341\n",
      " 342 343 344 345 346 347 348 350 351 352 353 354 355 356 358 359 361 362\n",
      " 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 379 380 381\n",
      " 382 384 386 387 389 390 391 393 394 395 396 397 398 399 400 401 402 403\n",
      " 404 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422\n",
      " 423 424 425 426 427 428 429 430 431 432 433 435 436 437 438 439 440 441\n",
      " 442 443 444 445 446 448 449 451 452 453 455 456 457 458 459 460 461 462\n",
      " 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480\n",
      " 481 482 483 484 485 486 487 488 489 491 492 493 494 495 496 497 498 499\n",
      " 500 501 502 503 504 505 506 507 508 509 510 512 513 514 515 516 517 518\n",
      " 519 520 523 524 525 526 527 528 529 532 533 534 535 536 537 538 539 540\n",
      " 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558\n",
      " 559 560 561 562 563 565 566 568]\n",
      "Test set:  [ 10  13  14  24  34  47  48  55  68  69  71  73  94 102 103 120 125 141\n",
      " 147 159 162 182 183 186 192 208 212 216 227 231 259 285 296 322 328 339\n",
      " 349 357 360 378 383 385 388 392 405 434 447 450 454 490 511 521 522 530\n",
      " 531 564 567]\n",
      "Train set:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  21  22  23  24  25  26  27  28  29  31  32  34  35  36  37  38  39\n",
      "  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  57  58\n",
      "  59  60  62  64  65  66  67  68  69  70  71  72  73  75  76  77  78  80\n",
      "  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99\n",
      " 100 101 102 103 104 105 106 108 109 110 111 112 113 114 115 116 117 118\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 134 135 136 137 138 139\n",
      " 140 141 142 143 144 145 146 147 148 150 151 152 154 156 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 173 174 175 176 177 178 179 180\n",
      " 181 182 183 185 186 187 188 189 190 191 192 193 196 197 199 200 202 203\n",
      " 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 222 223\n",
      " 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241\n",
      " 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 258 259 260\n",
      " 261 263 264 265 266 267 268 269 271 273 274 275 276 278 279 280 281 282\n",
      " 283 284 285 286 287 288 289 290 292 293 294 295 296 297 298 299 300 301\n",
      " 302 303 304 305 306 307 308 309 310 311 312 313 314 316 317 318 319 321\n",
      " 322 323 324 325 327 328 329 330 331 332 333 334 335 336 337 338 339 341\n",
      " 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n",
      " 361 362 363 364 365 366 368 369 370 371 372 373 374 375 376 377 378 379\n",
      " 381 382 383 384 385 386 387 388 389 390 391 392 393 394 396 397 398 399\n",
      " 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417\n",
      " 418 419 420 421 422 423 424 425 426 428 429 430 431 433 434 436 437 438\n",
      " 439 440 441 442 443 444 445 446 447 449 450 451 453 454 455 456 459 462\n",
      " 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480\n",
      " 481 483 484 485 486 487 488 490 491 492 493 494 495 496 497 498 499 500\n",
      " 501 502 503 504 505 506 507 508 509 510 511 513 514 516 517 518 519 520\n",
      " 521 522 523 524 525 526 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 556 557 558\n",
      " 559 561 562 563 564 565 566 567 568]\n",
      "Test set:  [ 19  20  30  33  56  61  63  74  79  83 107 119 132 133 149 153 155 157\n",
      " 172 184 194 195 198 201 204 221 257 262 270 272 277 291 315 320 326 340\n",
      " 342 367 380 395 427 432 435 448 452 457 458 460 461 482 489 512 515 527\n",
      " 555 560]\n",
      "Train set:  [  0   1   2   3   4   5   6   7   8   9  10  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  28  29  30  31  32  33  34  36  37  38\n",
      "  39  40  41  42  43  44  45  47  48  49  50  51  53  54  55  56  57  58\n",
      "  59  60  61  62  63  64  66  67  68  69  70  71  72  73  74  75  76  78\n",
      "  79  80  81  82  83  84  85  86  88  89  90  92  93  94  95  96  97  98\n",
      " 100 102 103 104 105 106 107 108 109 110 111 113 114 115 116 117 118 119\n",
      " 120 122 123 124 125 126 127 128 130 131 132 133 134 135 136 138 139 140\n",
      " 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158\n",
      " 159 160 162 163 164 165 166 167 169 170 171 172 173 175 176 177 179 180\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 199 201 203 204 205 206 207 208 209 210 211 212 213 215 216 217 218 219\n",
      " 220 221 222 223 225 226 227 228 229 230 231 232 233 234 235 236 237 238\n",
      " 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 256 257 258\n",
      " 259 260 261 262 263 264 265 266 267 269 270 271 272 273 274 275 276 277\n",
      " 278 279 280 281 282 284 285 286 288 289 290 291 292 294 295 296 297 299\n",
      " 300 301 302 303 304 306 307 308 309 311 312 313 314 315 316 317 318 319\n",
      " 320 321 322 323 324 326 328 329 330 331 332 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 347 349 350 351 352 353 354 355 356 357 358 359 360\n",
      " 361 362 364 365 366 367 368 369 370 372 373 374 375 376 378 379 380 381\n",
      " 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399\n",
      " 400 401 403 404 405 406 407 408 409 410 412 413 414 416 417 418 419 420\n",
      " 422 423 424 425 426 427 428 430 431 432 433 434 435 436 437 438 439 440\n",
      " 441 443 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460\n",
      " 461 462 463 464 465 466 467 468 469 470 471 472 473 474 476 477 478 480\n",
      " 481 482 483 484 485 486 487 488 489 490 491 492 493 495 496 498 499 500\n",
      " 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 518 519 520\n",
      " 521 522 523 524 525 526 527 528 529 530 531 532 533 535 536 537 538 539\n",
      " 540 541 542 543 545 546 547 548 549 550 551 552 553 555 556 557 558 559\n",
      " 560 561 562 563 564 565 566 567 568]\n",
      "Test set:  [ 11  27  35  46  52  65  77  87  91  99 101 112 121 129 137 161 168 174\n",
      " 178 200 202 214 224 254 255 268 283 287 293 298 305 310 325 327 333 346\n",
      " 348 363 371 377 402 411 415 421 429 442 444 475 479 494 497 501 517 534\n",
      " 544 554]\n",
      "Train set:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  22  23  24  25  26  27  28  29  30  32  33  34  35  36  38\n",
      "  39  40  41  43  44  46  47  48  50  51  52  53  54  55  56  57  58  59\n",
      "  60  61  62  63  64  65  66  67  68  69  71  72  73  74  75  76  77  78\n",
      "  79  80  81  82  83  84  85  87  88  89  90  91  92  93  94  95  96  97\n",
      "  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 116\n",
      " 117 118 119 120 121 122 123 124 125 126 128 129 130 131 132 133 134 135\n",
      " 136 137 138 139 140 141 142 143 144 145 146 147 149 150 151 152 153 154\n",
      " 155 157 158 159 160 161 162 163 164 165 166 168 169 170 171 172 173 174\n",
      " 175 176 177 178 179 181 182 183 184 186 187 188 189 190 191 192 193 194\n",
      " 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213 214 215 216 217 218 219 220 221 223 224 225 226 227 228 229 230 231\n",
      " 232 233 234 236 237 238 239 240 241 242 243 246 247 248 249 250 251 252\n",
      " 254 255 256 257 258 259 260 262 263 264 266 267 268 270 272 274 275 277\n",
      " 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 296 297\n",
      " 298 299 300 301 304 305 306 308 309 310 311 313 314 315 316 317 318 319\n",
      " 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337\n",
      " 339 340 341 342 343 344 345 346 347 348 349 352 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 370 371 372 373 374 375 377 378 379\n",
      " 380 381 382 383 384 385 386 387 388 389 390 391 392 394 395 396 397 399\n",
      " 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 418 419\n",
      " 420 421 422 423 424 426 427 428 429 430 431 432 433 434 435 436 438 439\n",
      " 440 441 442 443 444 446 447 448 449 450 451 452 453 454 455 456 457 458\n",
      " 459 460 461 462 463 465 466 467 468 469 470 471 473 474 475 476 479 480\n",
      " 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498\n",
      " 499 500 501 502 504 505 506 507 508 509 510 511 512 513 514 515 516 517\n",
      " 518 519 520 521 522 524 525 527 528 529 530 531 532 533 534 535 536 537\n",
      " 538 539 540 541 542 543 544 547 548 549 550 551 552 553 554 555 556 557\n",
      " 559 560 561 562 563 564 565 566 567]\n",
      "Test set:  [ 21  31  37  42  45  49  70  86 115 127 148 156 167 180 185 222 235 244\n",
      " 245 253 261 265 269 271 273 276 294 295 302 303 307 312 338 350 351 353\n",
      " 369 376 393 398 400 417 425 437 445 464 472 477 478 503 523 526 545 546\n",
      " 558 568]\n"
     ]
    }
   ],
   "source": [
    "# 10번의 실험을 위한 데이터 셋 구성을 살펴봄\n",
    "# 앞 실험과 다르게 데이터 인덱스가 섞여 있음을 확인할 수 있음\n",
    "for train_index, test_index in skf_sh.split(x, y):\n",
    "    print('Train set: ', train_index)\n",
    "    print('Test set: ', test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Fold Cross Validation Score\n",
      "[0.82758621 0.87931034 0.89473684 0.94736842 0.92982456 0.9122807\n",
      " 0.96491228 0.92857143 0.875      0.96428571]\n",
      "Average Accuracy\n",
      "0.9123876501598824\n"
     ]
    }
   ],
   "source": [
    "# DecisionTreeClassifier 모델을 clf에 생성\n",
    "clf = DecisionTreeClassifier()\n",
    "# Cross_val_score 함수를 사용해 x, y 데이터에 대해 10 fold cross validation 진행한 accuracy 출력\n",
    "scores = cross_val_score(clf, x, y, cv = skf_sh)\n",
    "# 10개 accuracy의 평균 출력\n",
    "print('K Fold Cross Validation Score')\n",
    "print(scores)\n",
    "print('Average Accuracy')\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단일선형회귀분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit learn package에서 linear_model이라는 모듈을 불러옴\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# R에서 사용하는 ggplot를 흉내내는 스타일 사용한다고 선언\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x^2</th>\n",
       "      <th>xy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>169</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>83</td>\n",
       "      <td>361</td>\n",
       "      <td>1577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>62</td>\n",
       "      <td>256</td>\n",
       "      <td>992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>48</td>\n",
       "      <td>196</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>58</td>\n",
       "      <td>225</td>\n",
       "      <td>870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>43</td>\n",
       "      <td>196</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x   y  x^2    xy\n",
       "0  13  40  169   520\n",
       "1  19  83  361  1577\n",
       "2  16  62  256   992\n",
       "3  14  48  196   672\n",
       "4  15  58  225   870\n",
       "5  14  43  196   602"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2차원 배열을 만들어 'data'라는 변수에 할당\n",
    "data = {'x' : [13, 19, 16, 14, 15, 14],\n",
    "        'y' : [40, 83, 62, 48, 58, 43],\n",
    "        'x^2' : [13*13,19*19,16*16,14*14,15*15,14*14],\n",
    "        'xy' : [13*40,19*83,16*62,14*48,15*58,14*43]}\n",
    "\n",
    "# data라는 변수의 값을 data frame 형태로 변환\n",
    "data = pd.DataFrame(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x   y\n",
       "0  13  40\n",
       "1  19  83\n",
       "2  16  62\n",
       "3  14  48\n",
       "4  15  58\n",
       "5  14  43"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2차원 배열을 만들어 'data'라는 변수에 할당\n",
    "data = {'x' : [13, 19, 16, 14, 15, 14],\n",
    "        'y' : [40, 83, 62, 48, 58, 43]}\n",
    "\n",
    "# data라는 변수의 값을 data frame 형태로 변환\n",
    "data = pd.DataFrame(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x28b17215f60>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot(kind='scatter', # 산점도를 그린다\n",
    "         x='x', # 가로축은 x라고 라벨\n",
    "         y='y', # 세로축은 y라고 라벨\n",
    "         figsize=(5,5), # 가로 5인치, 세로 5인치 크기의 박스를 설정\n",
    "         color='blue') # 산점도 상의 점 색상을 파랑 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear_model 모듈이 포함하고 있는 LinearRegression() 함수를 linear_regression 변수에 할당\n",
    "linear_regression = linear_model.LinearRegression()\n",
    "\n",
    "# LinearRegression()의 fit()이라는 함수를 이용하여 선형회귀모델 훈련 실행\n",
    "# 이 때 독립변수는 x, 종속변수는 y\n",
    "# fit 함수의 대문자 X 값은 TYPE이 DATAFRAME / 소문자 y는 SERIES\n",
    "linear_regression.fit(X = pd.DataFrame(data['x']), y = data['y'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "세로축 절편 a value =  -55.48175182481753\n",
      "기울기 b value =  [7.32846715]\n"
     ]
    }
   ],
   "source": [
    "# 선형회귀식의 세로축 절편 'linear_regression.intercept_'를 구하여 출력\n",
    "print('세로축 절편 a value = ', linear_regression.intercept_)\n",
    "\n",
    "# 선형회귀식의 기울기 'linear_regression.coef_'를 구하여 출력\n",
    "print('기울기 b value = ', linear_regression.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.211679\n",
      "1   -0.759124\n",
      "2    0.226277\n",
      "3    0.883212\n",
      "4    3.554745\n",
      "5   -4.116788\n",
      "Name: y, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    6.000000e+00\n",
       "mean     5.921189e-15\n",
       "std      2.491445e+00\n",
       "min     -4.116788e+00\n",
       "25%     -5.164234e-01\n",
       "50%      2.189781e-01\n",
       "75%      7.189781e-01\n",
       "max      3.554745e+00\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위에서 만들어진 선형회귀모델을 적용하여 선형회귀 값을 구해본다\n",
    "# 그 값을 prediction에 할당한다\n",
    "prediction = linear_regression.predict(X = pd.DataFrame(data['x']))\n",
    "\n",
    "# 실제 y값과 예측한 y값을 비교하여 잔차(residuals)를 구한다\n",
    "residuals = data['y'] - prediction\n",
    "print(residuals)\n",
    "\n",
    "# 변수의 갯수(6개), 잔차의 평균값, 잔차의 표준편차, 최소값, 25%값, 50% 값, 75% 값, 최대값을 출력\n",
    "residuals.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSE =  31.03649635036502\n",
      "SST =  1257.3333333333333\n",
      "R_squared =  0.9753156179610034\n"
     ]
    }
   ],
   "source": [
    "# 잔차를 제곱하여 전체를 합침, 결과값을 SSE라는 변수에 할당\n",
    "SSE = (residuals**2).sum()\n",
    "print('SSE = ', SSE)\n",
    "\n",
    "# y값의 표준편차를 제곱한 것을 모두 합침. 그 결과값을 SST라는 변수에 할당\n",
    "SST = ((data['y']-data['y'].mean())**2).sum()\n",
    "print('SST = ', SST)\n",
    "\n",
    "# 적합도 검증을 위해 필요한 결정계수 R을 구함\n",
    "R_squared = 1 - (SSE/SST)\n",
    "print('R_squared = ', R_squared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x28b173d8400>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot(kind=\"scatter\", x=\"x\", y=\"y\", figsize = (5, 5), color = \"red\")\n",
    "\n",
    "# Plot regression line\n",
    "plt.plot(data[\"x\"], prediction, color = \"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score =  0.9753156179610034\n",
      "Mean_Squared_Error =  5.172749391727503\n",
      "RMSE =  2.2743679103714736\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 결정계수 R값을 구함\n",
    "print('score = ', linear_regression.score(X = pd.DataFrame(data['x']), y = data['y']))\n",
    "\n",
    "# 실제값(data[y])과 회귀식 값(prediction)의 차이의 제곱을 구함\n",
    "print('Mean_Squared_Error = ', mean_squared_error(prediction, data['y']))\n",
    "\n",
    "# Mean squared error의 제곱근 값을 구함 / 평균제곱오차\n",
    "print('RMSE = ', mean_squared_error(prediction, data['y'])**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
